#!/usr/bin/env python3
"""
AI Providers –¥–ª—è VentAI MCP Server
–ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —Ä—ñ–∑–Ω–∏—Ö AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤: Ollama, Gemini, OpenAI, Anthropic
"""

import os
import logging
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

class AIProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–∏–π –∫–ª–∞—Å –¥–ª—è AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
    
    def __init__(self, name: str):
        self.name = name
        self.is_available = False
        
    @abstractmethod
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        pass
    
    @abstractmethod
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"""
        pass
    
    @abstractmethod
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö"""
        pass

class OllamaProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Ollama (–ª–æ–∫–∞–ª—å–Ω–∏–π AI)"""
    
    def __init__(self):
        super().__init__("ollama")
        self.base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        self.model = os.getenv('OLLAMA_MODEL', 'llama3.1')
        self.client = None
    
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Ollama –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            import ollama
            self.client = ollama.Client(host=self.base_url)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ
            models = self.client.list()
            available_models = [m['name'] for m in models['models']]
            
            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found in Ollama. Available: {available_models}")
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
                try:
                    self.client.pull(self.model)
                    logger.info(f"Successfully pulled {self.model}")
                except Exception as e:
                    logger.error(f"Failed to pull {self.model}: {e}")
                    return False
            
            self.is_available = True
            logger.info(f"‚úÖ Ollama provider initialized with model {self.model}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Ollama: {e}")
            return False
    
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Ollama"""
        if not self.is_available:
            raise Exception("Ollama provider not available")
        
        try:
            messages = []
            if context:
                system_msg = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç VentAI: {context.get('system_info', '')}"
                messages.append({"role": "system", "content": system_msg})
            
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": 0.7,
                    "top_p": 0.9
                }
            )
            
            return response['message']['content']
            
        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            raise
    
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Ollama"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π HVAC –¥–∞–Ω—ñ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç—É:
        - –ü–ª–æ—â–∞: {data.get('area', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')} –º¬≤
        - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª—é–¥–µ–π: {data.get('occupancy', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        - –ö–ª—ñ–º–∞—Ç–∏—á–Ω–∞ –∑–æ–Ω–∞: {data.get('climate_zone', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        - –ü–æ—Ç–æ—á–Ω–∞ —Å–∏—Å—Ç–µ–º–∞: {data.get('current_system', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        
        –ù–∞–¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑–≥—ñ–¥–Ω–æ –∑ –î–ë–ù –í.2.5-67:2013 —Ç–∞ –µ–Ω–µ—Ä–≥–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é.
        –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON.
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            return json.loads(response)
        except:
            return {
                "analysis": response,
                "provider": "ollama",
                "model": self.model
            }

class GeminiProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Google Gemini"""
    
    def __init__(self):
        super().__init__("gemini")
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
        self.client = None
    
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Gemini –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå GEMINI_API_KEY not found")
            return False
        
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(self.model_name)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            test_response = self.client.generate_content("Hello")
            if test_response.text:
                self.is_available = True
                logger.info(f"‚úÖ Gemini provider initialized with model {self.model_name}")
                return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Gemini: {e}")
            return False
        
        return False
    
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Gemini"""
        if not self.is_available:
            raise Exception("Gemini provider not available")
        
        try:
            full_prompt = prompt
            if context:
                system_info = context.get('system_info', '')
                full_prompt = f"{system_info}\n\n–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: {prompt}"
            
            response = self.client.generate_content(
                full_prompt,
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                }
            )
            
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            raise
    
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Gemini"""
        prompt = f"""
        –¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ HVAC —Å–∏—Å—Ç–µ–º –≤ –£–∫—Ä–∞—ó–Ω—ñ. –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ:
        
        üìä –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:
        - –ü–ª–æ—â–∞ –ø—Ä–∏–º—ñ—â–µ–Ω–Ω—è: {data.get('area', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')} –º¬≤
        - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª—é–¥–µ–π: {data.get('occupancy', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        - –ö–ª—ñ–º–∞—Ç–∏—á–Ω–∞ –∑–æ–Ω–∞: {data.get('climate_zone', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        - –ü–æ—Ç–æ—á–Ω–∞ —Å–∏—Å—Ç–µ–º–∞: {data.get('current_system', '–Ω–µ –≤–∫–∞–∑–∞–Ω–æ')}
        
        üéØ –ó–∞–≤–¥–∞–Ω–Ω—è:
        1. –†–æ–∑—Ä–∞—Ö—É–π –Ω–µ–æ–±—Ö—ñ–¥–Ω—É –ø–æ—Ç—É–∂–Ω—ñ—Å—Ç—å –∑–∞ –î–ë–ù –í.2.5-67:2013
        2. –ó–∞–ø—Ä–æ–ø–æ–Ω—É–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—É —Å–∏—Å—Ç–µ–º—É
        3. –û—Ü—ñ–Ω–∏ –µ–Ω–µ—Ä–≥–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
        4. –†–æ–∑—Ä–∞—Ö—É–π –æ—Ä—ñ—î–Ω—Ç–æ–≤–Ω—É –≤–∞—Ä—Ç—ñ—Å—Ç—å
        
        –í—ñ–¥–ø–æ–≤—ñ–¥—å —É JSON —Ñ–æ—Ä–º–∞—Ç—ñ:
        {{
            "recommended_system": "–Ω–∞–∑–≤–∞ —Å–∏—Å—Ç–µ–º–∏",
            "required_capacity": "–∫–í—Ç",
            "estimated_cost_usd": —á–∏—Å–ª–æ,
            "energy_efficiency": "A/B/C",
            "annual_savings_usd": —á–∏—Å–ª–æ,
            "compliance_dbn": true/false,
            "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è2"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            # –í–∏—Ç—è–≥—É—î–º–æ JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
        
        return {
            "analysis": response,
            "provider": "gemini",
            "model": self.model_name
        }

class OpenAIProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenAI"""
    
    def __init__(self):
        super().__init__("openai")
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.model = os.getenv('OPENAI_MODEL', 'gpt-4-turbo-preview')
        self.client = None
    
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå OPENAI_API_KEY not found")
            return False
        
        try:
            import openai
            self.client = openai.AsyncOpenAI(api_key=self.api_key)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            if response.choices:
                self.is_available = True
                logger.info(f"‚úÖ OpenAI provider initialized with model {self.model}")
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize OpenAI: {e}")
            return False
        
        return False
    
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ OpenAI"""
        if not self.is_available:
            raise Exception("OpenAI provider not available")
        
        try:
            messages = []
            if context:
                system_msg = f"–¢–∏ AI –∞—Å–∏—Å—Ç–µ–Ω—Ç VentAI. {context.get('system_info', '')}"
                messages.append({"role": "system", "content": system_msg})
            
            messages.append({"role": "user", "content": prompt})
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=2048
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI generation failed: {e}")
            raise
    
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ OpenAI"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π HVAC –ø—Ä–æ–µ–∫—Ç –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏:
        
        –î–∞–Ω—ñ: {data}
        
        –†–æ–∑—Ä–∞—Ö—É–π –∑–≥—ñ–¥–Ω–æ –∑ –î–ë–ù –í.2.5-67:2013 —ñ –ø–æ–≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É JSON:
        {{
            "recommended_system": "string",
            "required_capacity_kw": number,
            "estimated_cost_usd": number,
            "energy_class": "A+/A/B/C",
            "annual_savings_percent": number,
            "dbn_compliance": boolean,
            "recommendations": ["string"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            return json.loads(response)
        except:
            return {
                "analysis": response,
                "provider": "openai",
                "model": self.model
            }

class AnthropicProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Anthropic Claude"""
    
    def __init__(self):
        super().__init__("anthropic")
        self.api_key = os.getenv('ANTHROPIC_API_KEY')
        self.model = os.getenv('ANTHROPIC_MODEL', 'claude-3-sonnet-20240229')
        self.client = None
    
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Anthropic –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå ANTHROPIC_API_KEY not found")
            return False
        
        try:
            import anthropic
            self.client = anthropic.AsyncAnthropic(api_key=self.api_key)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            
            if response.content:
                self.is_available = True
                logger.info(f"‚úÖ Anthropic provider initialized with model {self.model}")
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Anthropic: {e}")
            return False
        
        return False
    
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Anthropic"""
        if not self.is_available:
            raise Exception("Anthropic provider not available")
        
        try:
            system_message = "–¢–∏ AI –µ–∫—Å–ø–µ—Ä—Ç VentAI –¥–ª—è HVAC —Å–∏—Å—Ç–µ–º –≤ –£–∫—Ä–∞—ó–Ω—ñ."
            if context:
                system_message += f" {context.get('system_info', '')}"
            
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=2048,
                temperature=0.7,
                system=system_message,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropic generation failed: {e}")
            raise
    
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Anthropic"""
        prompt = f"""
        –ï–∫—Å–ø–µ—Ä—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ HVAC —Å–∏—Å—Ç–µ–º–∏ –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏:
        
        üìã –î–∞–Ω—ñ –ø—Ä–æ–µ–∫—Ç—É:
        {data}
        
        üéØ –ó–∞–≤–¥–∞–Ω–Ω—è:
        1. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞ –î–ë–ù –í.2.5-67:2013
        2. –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏
        3. –û—Ü—ñ–Ω–∫–∞ –µ–Ω–µ—Ä–≥–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        4. –ï–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç —É JSON:
        {{
            "system_recommendation": "–¥–µ—Ç–∞–ª—å–Ω–∞ –Ω–∞–∑–≤–∞",
            "capacity_calculation": {{
                "required_kw": number,
                "peak_load_kw": number,
                "safety_factor": number
            }},
            "economic_analysis": {{
                "initial_cost_usd": number,
                "annual_operating_cost_usd": number,
                "savings_vs_baseline_percent": number,
                "payback_period_years": number
            }},
            "compliance": {{
                "dbn_67_2013": boolean,
                "energy_efficiency_class": "string",
                "co2_reduction_percent": number
            }},
            "detailed_recommendations": ["string"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            # –®—É–∫–∞—î–º–æ JSON –≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
        
        return {
            "analysis": response,
            "provider": "anthropic",
            "model": self.model
        }

class AIProviderManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
    
    def __init__(self):
        self.providers: Dict[str, AIProvider] = {}
        self.primary_provider = None
        
    async def initialize_all(self) -> Dict[str, bool]:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
        provider_classes = [
            OllamaProvider,
            GeminiProvider,
            OpenAIProvider,
            AnthropicProvider
        ]
        
        results = {}
        
        for provider_class in provider_classes:
            provider = provider_class()
            success = await provider.initialize()
            self.providers[provider.name] = provider
            results[provider.name] = success
            
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø–µ—Ä—à–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —è–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ
            if success and not self.primary_provider:
                self.primary_provider = provider.name
        
        return results
    
    def get_available_providers(self) -> List[str]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
        return [name for name, provider in self.providers.items() if provider.is_available]
    
    async def generate_with_provider(self, provider_name: str, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º"""
        if provider_name not in self.providers:
            raise ValueError(f"Provider {provider_name} not found")
        
        provider = self.providers[provider_name]
        if not provider.is_available:
            raise ValueError(f"Provider {provider_name} not available")
        
        return await provider.generate_response(prompt, context)
    
    async def analyze_hvac_with_provider(self, provider_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º"""
        if provider_name not in self.providers:
            raise ValueError(f"Provider {provider_name} not found")
        
        provider = self.providers[provider_name]
        if not provider.is_available:
            raise ValueError(f"Provider {provider_name} not available")
        
        return await provider.analyze_hvac_data(data)
    
    async def generate_with_fallback(self, prompt: str, context: Dict[str, Any] = None, preferred_provider: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ fallback –ª–æ–≥—ñ–∫–æ—é"""
        providers_to_try = []
        
        # –°–ø–æ—á–∞—Ç–∫—É —Å–ø—Ä–æ–±—É—î–º–æ –ø—Ä–µ—Ñ–µ—Ä–æ–≤–∞–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if preferred_provider and preferred_provider in self.providers:
            providers_to_try.append(preferred_provider)
        
        # –ü–æ—Ç—ñ–º –æ—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if self.primary_provider and self.primary_provider not in providers_to_try:
            providers_to_try.append(self.primary_provider)
        
        # –ü–æ—Ç—ñ–º –≤—Å—ñ —ñ–Ω—à—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ
        for name in self.get_available_providers():
            if name not in providers_to_try:
                providers_to_try.append(name)
        
        last_error = None
        
        for provider_name in providers_to_try:
            try:
                response = await self.generate_with_provider(provider_name, prompt, context)
                return {
                    "success": True,
                    "response": response,
                    "provider_used": provider_name
                }
            except Exception as e:
                last_error = e
                logger.warning(f"Provider {provider_name} failed: {e}")
                continue
        
        return {
            "success": False,
            "error": f"All providers failed. Last error: {last_error}",
            "providers_tried": providers_to_try
        }
