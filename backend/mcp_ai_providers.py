#!/usr/bin/env python3
"""
AI Providers –¥–ª—è VentAI MCP Server
–ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —Ä—ñ–∑–Ω–∏—Ö AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤: Ollama, Gemini, OpenAI, Anthropic
"""

import os
import logging
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod
import asyncio

# –Ü–º–ø–æ—Ä—Ç Redis –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
try:
    import redis
    import json
    import hashlib
    
    # Initialize Redis connection
    REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
    redis_client = redis.Redis.from_url(REDIS_URL, decode_responses=True)
    CACHE_EXPIRY = 3600  # 1 –≥–æ–¥–∏–Ω–∞
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("Redis not available - caching disabled")

logger = logging.getLogger(__name__)

def generate_cache_key(query):
    """–ì–µ–Ω–µ—Ä—É—î —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –∫–ª—é—á –∫–µ—à—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–ø–∏—Ç—É"""
    if not REDIS_AVAILABLE:
        return None
    query_str = json.dumps(query, sort_keys=True)
    return f"ai_cache:{hashlib.md5(query_str.encode()).hexdigest()}"

def get_cached_response(query):
    """–û—Ç—Ä–∏–º—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –∫–µ—à—É"""
    if not REDIS_AVAILABLE:
        return None
    cache_key = generate_cache_key(query)
    try:
        cached = redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
    except Exception as e:
        logger.warning(f"Error retrieving from cache: {e}")
    return None

def set_cached_response(query, response):
    """–ó–±–µ—Ä—ñ–≥–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤ –∫–µ—à"""
    if not REDIS_AVAILABLE:
        return
    cache_key = generate_cache_key(query)
    try:
        redis_client.setex(cache_key, CACHE_EXPIRY, json.dumps(response))
    except Exception as e:
        logger.warning(f"Error setting cache: {e}")

class AIProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–∏–π –∫–ª–∞—Å –¥–ª—è AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
    
    def __init__(self, name: str):
        self.name = name
        self.is_available = False
        
    @abstractmethod
    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        pass
    
    @abstractmethod
    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"""
        pass
    
    @abstractmethod
    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö"""
        pass

class OllamaProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Ollama (–ª–æ–∫–∞–ª—å–Ω–∏–π AI)"""
    
    def __init__(self):
        super().__init__("ollama")
        self.base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        self.model = os.getenv('OLLAMA_MODEL', 'llama3.1')
        self.client = None

    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Ollama –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            # –°–ø—Ä–æ–±—É—î–º–æ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ ollama
            import ollama
            self.client = ollama.Client(host=self.base_url)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ
            models = self.client.list()
            available_models = [m.model for m in models.models] if hasattr(models, 'models') else []
            
            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found in Ollama. Available: {available_models}")
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
                try:
                    self.client.pull(self.model)
                    logger.info(f"Successfully pulled {self.model}")
                except Exception as e:
                    logger.error(f"Failed to pull {self.model}: {e}")
                    return False
            
            self.is_available = True
            logger.info(f"‚úÖ Ollama provider initialized with model {self.model}")
            return True
            
        except ImportError:
            logger.error("‚ùå Ollama package not installed")
            return False
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Ollama: {e}")
            return False

    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Ollama"""
        if not self.is_available:
            raise Exception("Ollama provider not available")
        
        try:
            # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ –ø—Ä–æ–º–ø—Ç—É
            full_prompt = prompt
            if context and context.get('system_info'):
                full_prompt = f"–°–∏—Å—Ç–µ–º–∞: {context['system_info']}\n\n–ó–∞–ø–∏—Ç: {prompt}"
            
            response = self.client.generate(model=self.model, prompt=full_prompt)
            return response['response']
            
        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            raise

    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Ollama"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π HVAC —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–æ–µ–∫—Ç—É –≤ –£–∫—Ä–∞—ó–Ω—ñ:
        
        –î–∞–Ω—ñ: {data}
        
        –†–æ–∑—Ä–∞—Ö—É–π –∑–≥—ñ–¥–Ω–æ –∑ –î–ë–ù –í.2.5-67:2013 —Ç–∞ –ø–æ–≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
        - –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞
        - –ü–æ—Ç—É–∂–Ω—ñ—Å—Ç—å (–∫–í—Ç)
        - –û—Ä—ñ—î–Ω—Ç–æ–≤–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å (USD)
        - –ö–ª–∞—Å –µ–Ω–µ—Ä–≥–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        - –ï–∫–æ–Ω–æ–º—ñ—è –µ–Ω–µ—Ä–≥—ñ—ó (%)
        """
        
        response = await self.generate_response(prompt)
        return {
            "analysis": response,
            "provider": "ollama",
            "model": self.model
        }

class GeminiProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Google Gemini"""
    
    def __init__(self):
        super().__init__("gemini")
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model = os.getenv('GEMINI_MODEL', 'gemini-pro')
        self.client = None

    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Gemini –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå GEMINI_API_KEY not found")
            return False
        
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(self.model)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            response = self.client.generate_content("Hello")
            
            if response.text:
                self.is_available = True
                logger.info(f"‚úÖ Gemini provider initialized with model {self.model}")
                return True
                
        except ImportError:
            logger.error("‚ùå Google AI package not installed")
            return False
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Gemini: {e}")
            return False
        
        return False

    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Gemini"""
        if not self.is_available:
            raise Exception("Gemini provider not available")
        
        try:
            # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            full_prompt = prompt
            if context and context.get('system_info'):
                full_prompt = f"{context['system_info']}\n\n{prompt}"
            
            response = self.client.generate_content(full_prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            raise

    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Gemini"""
        prompt = f"""
        –ï–∫—Å–ø–µ—Ä—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ HVAC —Å–∏—Å—Ç–µ–º–∏ –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏:
        
        –î–∞–Ω—ñ –ø—Ä–æ–µ–∫—Ç—É: {data}
        
        –†–æ–∑—Ä–∞—Ö—É–π –∑–∞ –î–ë–ù –í.2.5-67:2013 —Ç–∞ –ø–æ–≤–µ—Ä–Ω–∏ JSON:
        {{
            "system_type": "—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞",
            "capacity_kw": —á–∏—Å–ª–æ,
            "cost_estimate_usd": —á–∏—Å–ª–æ,
            "efficiency_class": "A+/A/B/C",
            "annual_savings": —á–∏—Å–ª–æ_–≤_–ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö,
            "recommendations": ["—Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏—Ç—è–≥—Ç–∏ JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
        
        return {
            "analysis": response,
            "provider": "gemini",
            "model": self.model
        }

class OpenAIProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenAI"""
    
    def __init__(self):
        super().__init__("openai")
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.model = os.getenv('OPENAI_MODEL', 'gpt-4-turbo-preview')
        self.client = None

    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå OPENAI_API_KEY not found")
            return False
        
        try:
            import openai
            self.client = openai.AsyncOpenAI(api_key=self.api_key)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            if response.choices:
                self.is_available = True
                logger.info(f"‚úÖ OpenAI provider initialized with model {self.model}")
                return True
                
        except ImportError:
            logger.error("‚ùå OpenAI package not installed")
            return False
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize OpenAI: {e}")
            return False
        
        return False

    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ OpenAI"""
        if not self.is_available:
            raise Exception("OpenAI provider not available")
        
        try:
            messages = []
            if context:
                system_msg = f"–¢–∏ AI –∞—Å–∏—Å—Ç–µ–Ω—Ç VentAI. {context.get('system_info', '')}"
                messages.append({"role": "system", "content": system_msg})
            
            messages.append({"role": "user", "content": prompt})
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=2048
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI generation failed: {e}")
            raise

    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ OpenAI"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π HVAC –ø—Ä–æ–µ–∫—Ç –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏:
        
        –î–∞–Ω—ñ: {data}
        
        –†–æ–∑—Ä–∞—Ö—É–π –∑–≥—ñ–¥–Ω–æ –∑ –î–ë–ù –í.2.5-67:2013 —ñ –ø–æ–≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É JSON:
        {{
            "recommended_system": "string",
            "required_capacity_kw": number,
            "estimated_cost_usd": number,
            "energy_class": "A+/A/B/C",
            "annual_savings_percent": number,
            "dbn_compliance": boolean,
            "recommendations": ["string"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            return json.loads(response)
        except:
            return {
                "analysis": response,
                "provider": "openai",
                "model": self.model
            }

class AnthropicProvider(AIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Anthropic Claude"""
    
    def __init__(self):
        super().__init__("anthropic")
        self.api_key = os.getenv('ANTHROPIC_API_KEY')
        self.model = os.getenv('ANTHROPIC_MODEL', 'claude-3-sonnet-20240229')
        self.client = None

    async def initialize(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Anthropic –∫–ª—ñ—î–Ω—Ç–∞"""
        if not self.api_key:
            logger.error("‚ùå ANTHROPIC_API_KEY not found")
            return False
        
        try:
            import anthropic
            self.client = anthropic.AsyncAnthropic(api_key=self.api_key)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            
            if response.content:
                self.is_available = True
                logger.info(f"‚úÖ Anthropic provider initialized with model {self.model}")
                return True
                
        except ImportError:
            logger.error("‚ùå Anthropic package not installed")
            return False
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Anthropic: {e}")
            return False
        
        return False

    async def generate_response(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Anthropic"""
        if not self.is_available:
            raise Exception("Anthropic provider not available")
        
        try:
            system_message = "–¢–∏ AI –µ–∫—Å–ø–µ—Ä—Ç VentAI –¥–ª—è HVAC —Å–∏—Å—Ç–µ–º –≤ –£–∫—Ä–∞—ó–Ω—ñ."
            if context:
                system_message += f" {context.get('system_info', '')}"
            
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=2048,
                temperature=0.7,
                system=system_message,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropic generation failed: {e}")
            raise

    async def analyze_hvac_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ Anthropic"""
        prompt = f"""
        –ï–∫—Å–ø–µ—Ä—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ HVAC —Å–∏—Å—Ç–µ–º–∏ –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏:
        
        üìã –î–∞–Ω—ñ –ø—Ä–æ–µ–∫—Ç—É:
        {data}
        
        üéØ –ó–∞–≤–¥–∞–Ω–Ω—è:
        1. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞ –î–ë–ù –í.2.5-67:2013
        2. –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏
        3. –û—Ü—ñ–Ω–∫–∞ –µ–Ω–µ—Ä–≥–æ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        4. –ï–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç —É JSON:
        {{
            "system_recommendation": "–¥–µ—Ç–∞–ª—å–Ω–∞ –Ω–∞–∑–≤–∞",
            "capacity_calculation": {{
                "required_kw": number,
                "peak_load_kw": number,
                "safety_factor": number
            }},
            "economic_analysis": {{
                "initial_cost_usd": number,
                "annual_operating_cost_usd": number,
                "savings_vs_baseline_percent": number,
                "payback_period_years": number
            }},
            "compliance": {{
                "dbn_67_2013": boolean,
                "energy_efficiency_class": "string",
                "co2_reduction_percent": number
            }},
            "detailed_recommendations": ["string"]
        }}
        """
        
        response = await self.generate_response(prompt)
        
        try:
            import json
            # –®—É–∫–∞—î–º–æ JSON –≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
        
        return {
            "analysis": response,
            "provider": "anthropic",
            "model": self.model
        }

class AIProviderManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
    
    def __init__(self):
        self.providers: Dict[str, AIProvider] = {}
        self.primary_provider = None
        
    async def initialize_all(self) -> Dict[str, bool]:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
        provider_classes = [
            OllamaProvider,
            GeminiProvider,
            OpenAIProvider,
            AnthropicProvider
        ]
        
        results = {}
        
        for provider_class in provider_classes:
            provider = provider_class()
            success = await provider.initialize()
            self.providers[provider.name] = provider
            results[provider.name] = success
            
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø–µ—Ä—à–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —è–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ
            if success and not self.primary_provider:
                self.primary_provider = provider.name
        
        return results
    
    def get_available_providers(self) -> List[str]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤"""
        return [name for name, provider in self.providers.items() if provider.is_available]
    
    async def generate_with_provider(self, provider_name: str, prompt: str, context: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º"""
        if provider_name not in self.providers:
            raise ValueError(f"Provider {provider_name} not found")
        
        provider = self.providers[provider_name]
        if not provider.is_available:
            raise ValueError(f"Provider {provider_name} not available")
        
        return await provider.generate_response(prompt, context)
    
    async def analyze_hvac_with_provider(self, provider_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ HVAC –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º"""
        if provider_name not in self.providers:
            raise ValueError(f"Provider {provider_name} not found")
        
        provider = self.providers[provider_name]
        if not provider.is_available:
            raise ValueError(f"Provider {provider_name} not available")
        
        return await provider.analyze_hvac_data(data)
    
    async def generate_with_fallback(self, prompt: str, context: Dict[str, Any] = None, preferred_provider: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ fallback –ª–æ–≥—ñ–∫–æ—é"""
        providers_to_try = []
        
        # –°–ø–æ—á–∞—Ç–∫—É —Å–ø—Ä–æ–±—É—î–º–æ –ø—Ä–µ—Ñ–µ—Ä–æ–≤–∞–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if preferred_provider and preferred_provider in self.providers:
            providers_to_try.append(preferred_provider)
        
        # –ü–æ—Ç—ñ–º –æ—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if self.primary_provider and self.primary_provider not in providers_to_try:
            providers_to_try.append(self.primary_provider)
        
        # –ü–æ—Ç—ñ–º –≤—Å—ñ —ñ–Ω—à—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ
        for name in self.get_available_providers():
            if name not in providers_to_try:
                providers_to_try.append(name)
        
        last_error = None
        
        for provider_name in providers_to_try:
            try:
                response = await self.generate_with_provider(provider_name, prompt, context)
                return {
                    "success": True,
                    "response": response,
                    "provider_used": provider_name
                }
            except Exception as e:
                last_error = e
                logger.warning(f"Provider {provider_name} failed: {e}")
                continue
        
        return {
            "success": False,
            "error": f"All providers failed. Last error: {last_error}",
            "providers_tried": providers_to_try
        }

# Backward compatibility function
def chat_with_ai(query):
    """–§—É–Ω–∫—Ü—ñ—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
    cached_response = get_cached_response(query)
    if cached_response:
        print("Returning cached AI response")
        return cached_response

    # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤ –∫–µ—à—ñ, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –±–∞–∑–æ–≤—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
    response = {"reply": f"AI analysis for: {query}", "suggestions": ["Check system efficiency"]}

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
    set_cached_response(query, response)
    return response